{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Verification for SisFall Dataset\n",
    "\n",
    "This notebook verifies that the SisFall data preprocessing pipeline adheres to the `DATA_SPLITTING_GUIDE.md` requirements, especially concerning data splitting and impact-zone-based labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data directory: /Users/minhphan/src/Fall-TSAD/data/processed/sisfall\n",
      "Raw data directory: /Users/minhphan/src/Fall-TSAD/data/raw/sisfall\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths (adjust if necessary)\n",
    "PROCESSED_DIR = Path('../data/processed/sisfall/')\n",
    "RAW_DIR = Path('../data/raw/sisfall/')\n",
    "SAMPLING_RATE = 200 # Hz, as per configs/data/sisfall.yaml\n",
    "\n",
    "print(f\"Processed data directory: {PROCESSED_DIR.resolve()}\")\n",
    "print(f\"Raw data directory: {RAW_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Split Files and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata Head:\n",
      "           filename code subject  group trial  is_fall  \\\n",
      "0  D01_SA01_R01.txt  D01    SA01  young   R01        0   \n",
      "1  D02_SA01_R01.txt  D02    SA01  young   R01        0   \n",
      "2  D03_SA01_R01.txt  D03    SA01  young   R01        0   \n",
      "3  D04_SA01_R01.txt  D04    SA01  young   R01        0   \n",
      "4  D05_SA01_R01.txt  D05    SA01  young   R01        0   \n",
      "\n",
      "                                                path  \n",
      "0  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "1  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "2  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "3  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "4  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "\n",
      "Train Files Head:\n",
      "           filename code subject  group trial  is_fall  \\\n",
      "0  D08_SA10_R02.txt  D08    SA10  young   R02        0   \n",
      "1  D15_SA02_R02.txt  D15    SA02  young   R02        0   \n",
      "2  D15_SA07_R04.txt  D15    SA07  young   R04        0   \n",
      "3  D06_SA21_R05.txt  D06    SA21  young   R05        0   \n",
      "4  D13_SA06_R05.txt  D13    SA06  young   R05        0   \n",
      "\n",
      "                                                path  \n",
      "0  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "1  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "2  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "3  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "4  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "\n",
      "Validation Files Head:\n",
      "           filename code subject  group trial  is_fall  \\\n",
      "0  D06_SA13_R05.txt  D06    SA13  young   R05        0   \n",
      "1  D07_SA06_R05.txt  D07    SA06  young   R05        0   \n",
      "2  D06_SA13_R02.txt  D06    SA13  young   R02        0   \n",
      "3  D01_SA10_R01.txt  D01    SA10  young   R01        0   \n",
      "4  D16_SA20_R01.txt  D16    SA20  young   R01        0   \n",
      "\n",
      "                                                path  \n",
      "0  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "1  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "2  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "3  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "4  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "\n",
      "Test Files Head:\n",
      "           filename code subject  group trial  is_fall  \\\n",
      "0  F10_SA05_R03.txt  F10    SA05  young   R03        1   \n",
      "1  F15_SA13_R03.txt  F15    SA13  young   R03        1   \n",
      "2  F02_SA06_R03.txt  F02    SA06  young   R03        1   \n",
      "3  F07_SA08_R04.txt  F07    SA08  young   R04        1   \n",
      "4  F05_SA08_R04.txt  F05    SA08  young   R04        1   \n",
      "\n",
      "                                                path  \n",
      "0  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "1  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "2  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "3  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "4  /Users/minhphan/src/Fall-TSAD/data/raw/sisfall...  \n",
      "\n",
      "Unique subjects in Train: 23\n",
      "Unique subjects in Validation: 23\n",
      "Unique subjects in Test: 38\n",
      "\n",
      "Classes in Train: ['young']\n",
      "Classes in Validation: ['young']\n",
      "Classes in Test: ['young' 'elderly']\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and split files\n",
    "metadata_df = pd.read_csv(PROCESSED_DIR / 'metadata.csv')\n",
    "train_files_df = pd.read_csv(PROCESSED_DIR / 'splits/train.csv')\n",
    "val_files_df = pd.read_csv(PROCESSED_DIR / 'splits/val.csv')\n",
    "test_files_df = pd.read_csv(PROCESSED_DIR / 'splits/test.csv')\n",
    "\n",
    "print(\"Metadata Head:\")\n",
    "print(metadata_df.head())\n",
    "print(\"\\nTrain Files Head:\")\n",
    "print(train_files_df.head())\n",
    "print(\"\\nValidation Files Head:\")\n",
    "print(val_files_df.head())\n",
    "print(\"\\nTest Files Head:\")\n",
    "print(test_files_df.head())\n",
    "\n",
    "# print unique subjects in each split\n",
    "print(f\"\\nUnique subjects in Train: {train_files_df['subject'].nunique()}\")\n",
    "print(f\"Unique subjects in Validation: {val_files_df['subject'].nunique()}\")\n",
    "print(f\"Unique subjects in Test: {test_files_df['subject'].nunique()}\")\n",
    "\n",
    "# print number of classes of groups in each split\n",
    "print(f\"\\nClasses in Train: {train_files_df['group'].unique()}\")\n",
    "print(f\"Classes in Validation: {val_files_df['group'].unique()}\")\n",
    "print(f\"Classes in Test: {test_files_df['group'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Checks for Split Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set composition: OK (young ADL only)\n",
      "Validation set composition: OK (young ADL + some young FALLs)\n",
      "Test set composition: OK (young + elderly ADL + FALLs)\n",
      "File overlap: OK (no overlap between splits)\n"
     ]
    }
   ],
   "source": [
    "# Check 1: Train set contains only young ADL\n",
    "assert all(train_files_df['group'] == 'young'), \"Train set contains non-young participants.\"\n",
    "assert all(train_files_df['is_fall'] == 0), \"Train set contains fall events.\"\n",
    "print(\"Train set composition: OK (young ADL only)\")\n",
    "\n",
    "# Check 2: Validation set contains young ADL and some young FALLs\n",
    "assert all(val_files_df['group'] == 'young'), \"Validation set contains non-young participants.\"\n",
    "assert any(val_files_df['is_fall'] == 1), \"Validation set does not contain fall events.\"\n",
    "assert any(val_files_df['is_fall'] == 0), \"Validation set does not contain ADL events.\"\n",
    "print(\"Validation set composition: OK (young ADL + some young FALLs)\")\n",
    "\n",
    "# Check 3: Test set contains young + elderly ADL + FALLs\n",
    "assert any(test_files_df['group'] == 'young'), \"Test set does not contain young participants.\"\n",
    "assert any(test_files_df['group'] == 'elderly'), \"Test set does not contain elderly participants.\"\n",
    "assert any(test_files_df['is_fall'] == 1), \"Test set does not contain fall events.\"\n",
    "assert any(test_files_df['is_fall'] == 0), \"Test set does not contain ADL events.\"\n",
    "print(\"Test set composition: OK (young + elderly ADL + FALLs)\")\n",
    "\n",
    "# Check 4: No file overlap between splits\n",
    "train_paths = set(train_files_df['path'])\n",
    "val_paths = set(val_files_df['path'])\n",
    "test_paths = set(test_files_df['path'])\n",
    "\n",
    "assert len(train_paths.intersection(val_paths)) == 0, \"Overlap between train and validation sets.\"\n",
    "assert len(train_paths.intersection(test_paths)) == 0, \"Overlap between train and test sets.\"\n",
    "assert len(val_paths.intersection(test_paths)) == 0, \"Overlap between validation and test sets.\"\n",
    "print(\"File overlap: OK (no overlap between splits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Window Labels for Fall Events (Impact Zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying labeling for fall event: /Users/minhphan/src/Fall-TSAD/data/raw/sisfall/SA22/F05_SA22_R02.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string ' 158;' to float64 at row 0, column 9.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load raw signal (assuming it's in the raw_dir and has a .txt extension)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m raw_signal_path \u001b[38;5;241m=\u001b[39m RAW_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfall_subject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfall_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfall_subject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfall_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_signal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load corresponding segmented labels (this requires finding the index of this file in the val_data.npy)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This is a bit tricky as we don't have a direct mapping from file to index in the .npy array.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# For simplicity, we'll re-run the segmentation logic for this specific file to get its labels.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# In a real scenario, you might save a mapping or load the full val_labels.npy and find the relevant section.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# For now, let's assume we can load the entire val_data.npy and val_labels.npy\u001b[39;00m\n\u001b[1;32m     21\u001b[0m val_data_segmented \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(PROCESSED_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fall-tsad/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1395\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1393\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1395\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fall-tsad/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1046\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1046\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string ' 158;' to float64 at row 0, column 9."
     ]
    }
   ],
   "source": [
    "# Load segmented data and labels for a fall event from the validation set\n",
    "# Find a fall event in the validation set\n",
    "fall_val_entry = val_files_df[val_files_df['is_fall'] == 1].iloc[0]\n",
    "fall_file_path = fall_val_entry['path']\n",
    "fall_subject = fall_val_entry['subject']\n",
    "fall_code = fall_val_entry['code']\n",
    "fall_trial = fall_val_entry['trial']\n",
    "\n",
    "print(f\"Verifying labeling for fall event: {fall_file_path}\")\n",
    "\n",
    "# Load raw signal (assuming it's in the raw_dir and has a .txt extension)\n",
    "raw_signal_path = RAW_DIR / f\"{fall_subject}/{fall_code}_{fall_subject}_{fall_trial}.txt\"\n",
    "raw_data = np.loadtxt(raw_signal_path, delimiter=',')\n",
    "\n",
    "# Load corresponding segmented labels (this requires finding the index of this file in the val_data.npy)\n",
    "# This is a bit tricky as we don't have a direct mapping from file to index in the .npy array.\n",
    "# For simplicity, we'll re-run the segmentation logic for this specific file to get its labels.\n",
    "# In a real scenario, you might save a mapping or load the full val_labels.npy and find the relevant section.\n",
    "\n",
    "# For now, let's assume we can load the entire val_data.npy and val_labels.npy\n",
    "val_data_segmented = np.load(PROCESSED_DIR / 'val_data.npy')\n",
    "val_labels_segmented = np.load(PROCESSED_DIR / 'val_labels.npy')\n",
    "\n",
    "# To properly verify, we need to re-segment this specific file and compare.\n",
    "# This requires the original `segment_data` and `segment_dataset` logic.\n",
    "# For a quick check, we can just plot the labels from `val_labels_segmented` if we know the range.\n",
    "\n",
    "# Let's simplify: we'll just plot the raw data and visually inspect where labels '1' should be.\n",
    "# To get the actual labels for this specific file, we would need to re-run the segmentation\n",
    "# or have a more sophisticated way to extract them from the concatenated arrays.\n",
    "\n",
    "# For a more robust check, we'd need to integrate the `segment_dataset` function here\n",
    "# or modify `serialize` to save individual file labels before concatenation.\n",
    "\n",
    "# For now, let's just plot the raw data and the magnitude to identify the impact zone.\n",
    "magnitude = np.sqrt(np.sum(raw_data**2, axis=1))\n",
    "impact_idx = np.argmax(magnitude)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(raw_data[:, 0], label='Acc X')\n",
    "plt.plot(raw_data[:, 1], label='Acc Y')\n",
    "plt.plot(raw_data[:, 2], label='Acc Z')\n",
    "plt.plot(magnitude, label='Magnitude', linestyle='--', color='black')\n",
    "plt.axvline(x=impact_idx, color='red', linestyle=':', label='Impact Peak')\n",
    "plt.axvspan(impact_idx - 0.5 * SAMPLING_RATE, impact_idx + 1.0 * SAMPLING_RATE, color='red', alpha=0.2, label='Impact Zone (0.5s pre, 1.0s post)')\n",
    "plt.title(f'Raw Accelerometer Data and Impact Zone for Fall Event: {fall_code}')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Acceleration (g)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Visual inspection required: Check if the impact zone aligns with the expected fall event.\")\n",
    "print(\"To fully verify window labels, a more complex setup is needed to map segmented labels back to original files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Check for Train Labels (All 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(PROCESSED_DIR / 'train_labels.npy')\n",
    "assert np.all(train_labels == 0), \"Train labels contain non-zero values (fall events).\"\n",
    "print(\"Train labels verification: OK (all 0s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fall-tsad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
